---
title: Syntiant Core 2
date: 2021-01-22 13:00
slug: SyntiantCore2
category: Syntiant
tags: ["Deep Learning", "Chip Design", "Architectures", "AI", "Energy Efficiency", "Low Power"]
summary: Among the high points in my career is shipping a new neural accelerator with the startup Syntiant. This blog post is how I portrayed the effort at the time.
icon: FaMicrochip
---

> Update: my Syntiant Core 2 [presentation](https://youtu.be/uqR2MjucNw4?t=3070) bested Samsung and Qualcomm to win [Best Product at TinyML](https://www.syntiant.com/post/syntiant-ndp120-awarded-best-product-of-the-year-at-tinyml-summit)!

<div className="row">
  &nbsp;
  <br />
  <div className="col-xs-12 col-sm-12 col-md-9">
    <p>
      One of my great professional privileges when founding Syntiant's machine
      learning stack has been collaborating with a team of electrical engineers
      in the design, development, verification, and shipping of an ultra-power
      efficient tensor computation core for Syntiant products. Although I
      resisted interviewing with Syntiant because I am not an electrical
      engineer, the hardware collaboration process we have developed through now
      two "Syntiant Cores" has borne fruit after three years of trading between
      machine learning requirements and silicon capabilities.
    </p>
    <p>
      I am quite proud of the silicon architecture we developed together and
      impressed with the capabilities of my electrical engineering colleagues.
      Never did they say to me "it can't be done." Below is a marketing post I
      wrote for machine learning engineers as an introduction of Syntiant's now
      two cores.
    </p>
  </div>
  <div className="col-xs-12 col-sm-12 col-md-3">
    <a href="https://apnews.com/press-release/globenewswire-mobile/technology-business-corporate-news-products-and-services-industrial-products-and-services-18653b4146a5b74fbaa5114f570f8dc6">
      <img
        width="70%"
        style={{ maxWidth: 200 }}
        src="/static/images/blog/content/ndp120/honoree.png"
      />
    </a>
  </div>
</div>
---

# The Growing Syntiant Core Family

With the [release of the NDP120](https://www.syntiant.com/ndp120), Syntiant's line of Neural Decision Processors (NDPs) now include two tensor computation cores providing superior energy efficiencies in neural computation. The "Syntiant Core 2" builds on the lessons learned in the field from [millions of shipped](https://www.syntiant.com/post/syntiant-surpasses-milestone-of-10-million-processors-shipped) [Syntiant Core 1](https://www.syntiant.com/ndp100) found in the NDP100 line of chips. While the Syntiant Core 1 requires the absolute lowest energy running at 140 uW for audio key-word-spotting applications, the Syntiant Core 2 provides a flexible neural network runtime with up to 25x the processing power of the Syntiant Core 1. Here we cover the details of both Syntiant cores to explore the problems solvable by the cores and the modelling effort required to put them into the field.

<figure>
  <div className="row">
    &nbsp;
    <br />
    <div className="col-xs-12 col-sm-12 col-md-12">
      <div className="row">
        <div className="col-xs-6 col-sm-6 col-md-6">
          <img
            id="img3"
            src="/static/images/blog/content/ndp120/speeder.png"
            className="img-responsive img-rounded"
          />
          <p>Figure 1a. The SC1 navigating a forest</p>
        </div>
        <div className="col-xs-6 col-sm-6 col-md-6">
          <img
            id="img"
            src="/static/images/blog/content/ndp120/xwing.png"
            className="img-responsive img-rounded"
          />
          <p>Figure 1b. The SC2 flying over "competing" edge processors</p>
        </div>
      </div>
      <div className="row">
        <div className="col-xs-12 col-sm-12 col-md-12">
          <img
            id="img2"
            src="/static/images/blog/content/ndp120/starkiller.png"
            className="img-responsive img-rounded"
          />
          <p>
            Figure 1c. A charged GPU shortly after draining the nearest star for
            power
          </p>
        </div>
      </div>
    </div>
  </div>
</figure>


## Syntiant Core 2


<>
  <div className="row">
    &nbsp;
    <br />
    <div className="col-xs-12 col-sm-12 col-md-6">
      <p>
        Edge neural network chips are notoriously difficult training targets.
        The combination of limited neural architecture support, high compression
        factors, and distributional shift between training data and the real
        world can delay product shipments for months or even years. When
        designing the Syntiant Core, Syntiant sought to alleviate these
        challenges and enable more and better edge neural solutions on tight
        production timelines.
      </p>
      <p>
        As a first principle, the SC2's highly-optimized tensor-based memory
        subsystem is designed to avoid inefficiencies in stored program
        architectures. Each layer independently controls its parameter, input,
        and output tensors consistent with graph-based execution and runs
        without repeated memory addressing overhead. The graph-based execution
        provides full control for running concurrent independent networks, or
        even swapping network configurations depending on operating conditions.
        When building on top of the SC2 execution engine, many always-on problem
        domains become feasible because you don't pay for the inefficiencies of
        a microcontroller. The benefits of specialization are manifest not only
        in power efficiency, but also in throughput.
      </p>
    </div>
    <div className="col-xs-12 col-sm-12 col-md-6">
      <figure>
        <img
          id="img0"
          src="/static/images/blog/content/ndp120/storedprogram.png"
          className="img-responsive img-rounded"
        />
        <p>
          Figure 2. Stored program architectures have their programs stored in
          memory and are loaded at runtime. Consequently, the runtime cannot
          make many program assumptions that optimize for throughput and energy.
          In contrast, the SC2 has all the neural network programming baked into
          the silicon.
        </p>
      </figure>
    </div>
  </div>
  <div className="row">
    &nbsp;
    <br />
    <div className="col-xs-12 col-sm-12 col-md-6">
      <p>
        The SC2 packs 32 parallel multiply and accumulate (MAC) paths operating
        at up to 100 MHz. This packs enough solution power to continuously
        process audio signal paths, detect objects in images, and monitor an
        accelerometer for motion events. These inputs can be concatenated for
        single models that jointly understand audio, image, and motion of the
        device's environment, or cascaded into multi-modal conditional
        workloads. With the ability to grow through time, the SC2 can serve as
        the basis for multi-product development programs that solve new deep
        learning problems with each generation.
      </p>
      <p>
        While the SC2 has the throughput necessary for many problem domains, can
        it express proven and forthcoming neural architectures that solve tasks
        at production environment levels?
      </p>
    </div>
    <div className="col-xs-12 col-sm-12 col-md-6">
      <div className="card">
        <div className="card-body">
          <div className="card-header">
            <h5 className="card-title">Problem domains</h5>
          </div>
          <ul className="card-text">
            <li>Far field wakeword</li>
            <li>Multi-modal sensor fusion</li>
            <li>Speech enhancement</li>
            <li>Active noise cancellation</li>
            <li>Speech interfaces</li>
            <li>Acoustic event detection</li>
            <li>Image recognition</li>
            <li>Gesture detection</li>
            <li>Anomaly detection</li>
            <li>Passive infrared event detection</li>
          </ul>
        </div>
      </div>
    </div>
  </div>
  <div className="row">
    &nbsp;
    <br />
    <div className="col-xs-12 col-sm-12 col-md-6">
      <p>
        After spending months or years learning how to train specific
        architectures to production standards, deep learning engineers do not
        want to throw out their playbook and start with an architecture dictated
        by silicon. With the SC2, the deep learning engineers can select their
        neural architecture. Syntiant cores are highly-optimized for tensor
        computation and can run many architectures end to end without needing to
        involve any of the stored program components of the NDPs. For more
        exotic architectures, the SC2 shares execution with the on-chip digital
        signal processor (DSP). So the SC2 can, for example, compute a large
        convolutional kernel in parallel while the DSP runs a leaky ReLu
        activation.{" "}
        <span style={{ fontWeight: "bold" }}>
          Port a trained network or move flexibly through the neural design
          space.
        </span>
      </p>
      <p>
        While supporting so many different architectures makes a wide variety of
        solutions possible, it is also important that these architectures be
        trainable from the data and tools that are already well known by deep
        learning engineers.
      </p>
      <p>
        All major frameworks and interchange formats port to the SC2 runtime.
        Whether the SC2 runs inference end to end, or the DSP is involved,
        Syntiant's tools ship with a bit-exact GPU-accelerated runtime for
        evaluating performance.{" "}
        <span style={{ fontWeight: "bold" }}>
          Deep learning engineers do not need to leave the cloud
        </span>{" "}
        to evaluate on the edge. You can test against terabytes of data without
        maxing out your cloud budgets.
      </p>
    </div>
    <div className="col-xs-12 col-sm-12 col-md-6">
      <div className="card">
        <div className="card-body">
          <div className="card-header">
            <h5 className="card-title">Neural Architecture Support</h5>
          </div>
          <ul className="card-text">
            <li>Native Layers</li>
            <ul>
              <li>Convolutions: Standard, Upsampling, Depthwise</li>
              <li>Pooling: Average, Max</li>
              <li>Fully connected: Dense</li>
              <li>Operations: Multiplication, addition</li>
            </ul>
            <li>Compositional Layers (multiple native)</li>
            <ul>
              <li>LSTM</li>
              <li>GRU</li>
            </ul>
            <li>Native Activations</li>
            <ul>
              <li>ReLu</li>
              <li>TanH</li>
              <li>Sigmoid</li>
              <li>Linear</li>
            </ul>
            <li>Architecture Types</li>
            <ul>
              <li>Convolutions with striding and dilation</li>
              <li>Ensembles and cascades</li>
              <li>Attention</li>
              <li>Skip connections</li>
              <li>Branching/concatenating layers</li>
              <li>And more!</li>
            </ul>
          </ul>
        </div>
      </div>
    </div>
  </div>
  <div className="row">
    &nbsp;
    <br />
    <div className="col-xs-12 col-sm-12 col-md-6">
      <p>
        Although support for "no compromises" neural design is an important
        feature of the SC2, it is also possible to get closer to the metal with
        design tools supplied by Syntiant. The complete power, latency, memory,
        and parameter requirements of different neural architectures can be
        scored programmatically and hooked into large scale{" "}
        <span style={{ fontWeight: "bold" }}>
          hardware-aware hyperparameter searches
        </span>
        . Most hyperparameter searches will find task performance is stronger
        with larger architectures, but these architectures may be so large that
        they need to be compressed.
      </p>
      <p>
        The evolving practice of neural compression is natively supported by the
        SC2. Architectures support mixtures of 1, 2, 4, and 8 bit weights with
        higher precision bias terms. For the most challenging tasks, the SC2
        supports high precision modes including 16 bit inputs and outputs. Heavy
        quantization is optional.{" "}
        <span style={{ fontWeight: "bold" }}>
          Quantize when wanted, but not just because an edge processor requires
          it.
        </span>
      </p>
    </div>
    <div className="col-xs-12 col-sm-12 col-md-6">
      <div className="card">
        <div className="card-body">
          <div className="card-header">
            <h5 className="card-title">Framework Support</h5>
          </div>
          <ul className="card-text">
            <li>Native Support</li>
            <ul>
              <li>Keras Floating Point</li>
              <li>Keras Fixed Point (bit-exact for chip)</li>
            </ul>
            <li>Easy Sourcing</li>
            <ul>
              <li>Keras</li>
              <li>Tensorflow</li>
              <li>PyTorch</li>
              <li>MXNet</li>
              <li>h5</li>
              <li>ONNX</li>
            </ul>
            <li>Native Activations</li>
            <ul>
              <li>ReLu</li>
              <li>TanH</li>
              <li>Sigmoid</li>
              <li>Linear</li>
            </ul>
          </ul>
        </div>
      </div>
    </div>
  </div>
  <div className="row">
    &nbsp;
    <br />
    <div className="col-xs-12 col-sm-12 col-md-6">
      <p>
        While all these features are general to any neural network solution, the
        SC2 includes task-specific optimizations that exploit the properties of
        time series and sparsity to increase throughput and decrease energy
        consumption.
      </p>
    </div>
    <div className="col-xs-12 col-sm-12 col-md-6">
      <div className="card">
        <div className="card-body">
          <div className="card-header">
            <h5 className="card-title">Neural Compression Support</h5>
          </div>
          <ul className="card-text">
            <li>Mix 1, 2, 4, and 8 bit precision layers</li>
            <li>Bias term is always 16 bits</li>
            <li>Switch between 8 and 16 bit activations</li>
            <li>Modify activation scales to minimize quantization error</li>
            <li>Prune activations</li>
          </ul>
        </div>
      </div>
    </div>
  </div>
  <div className="row">
    &nbsp;
    <br />
    <div className="col-xs-12 col-sm-12 col-md-6">
      <p>
        These optimizations run "under the hood," meaning the SC2 leverages
        these optimizations without the deep learning engineer needing to tune
        or condition them at training time. The solution just works better.
      </p>
    </div>
    <div className="col-xs-12 col-sm-12 col-md-6">
      <div className="card">
        <div className="card-body">
          <div className="card-header">
            <h5 className="card-title">Task-Specific Optimizations</h5>
          </div>
          <ul className="card-text">
            <li>
              Avoid recomputing convolutional kernels in time series with
              caching
            </li>
            <li>Automatically skip computations of sparse inputs</li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</>

## Syntiant Core 1

> Perfection is achieved, not when there is nothing more to add, but when there is nothing left to take away.
> - Antoine de Saint-Exupéry
 

<div className="row">
  &nbsp;
  <br />
  <div className="col-xs-12 col-sm-12 col-md-6">
    <p>
      While the SC2 is multi-featured and flexible, the SC1 is always a five
      layer fully connected network of 4 bit weights and biases.
    </p>
    <p>
      This network scale is scoped to meet near field wakeword requirements
      (e.g., "Alexa" and "OK Google") via 8 parallel MACs operating at 16 MHz.
      Many single-network tasks fit within this network size, including many
      audio processing tasks that operate on extracted audio features.
    </p>
  </div>
  <div className="col-xs-12 col-sm-12 col-md-6">
    <div className="card">
      <div className="card-body">
        <div className="card-header">
          <h5 className="card-title">Example Problem Domains</h5>
        </div>
        <ul className="card-text">
          <li>Near field wakeword and small vocabulary speech interfaces</li>
          <li>Acoustic event detection</li>
          <li>Sensor event detection</li>
          <li>Passive infrared event detection</li>
        </ul>
      </div>
    </div>
  </div>
</div>


How should you choose between the Syntiant cores? Thankfully, you don't need to choose between the SC1 and 2 before beginning deep learning engineering. The same tools developed for the SC2 have all been backported to the SC1, so if you find your task is solvable with a smaller neural network, you can switch to the SC1 in Syntiant's tools with just a single line of code. Start with the SC1 and scale up if needed, or start with the SC2 and scale down when the solution more than exceeds expectations. In either event, <span style={{fontWeight: "bold"}}>deep learning solutions now have room to grow.</span>

---

Note: This post has been updated from the [version originally posted](/pages/SyntiantCore2-old.html) to update the star wars graphic, add a new graphic illustrating stored program architectures, and clarified a few points.
