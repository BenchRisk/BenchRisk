---
title: The Problem with Priors
draft: true
date: 2023-3-28 13:00
category: Ethical Computing
tags: ["Machine Learning", "Ethics", "AI", "Safety"]
summary: Whenever people ask me whether it is possible to make an unbiased machine learning system, I strain to explain that the answer depends on the intent of the question. We can make "good" machine learning systems, but the problem of priors means we will never make an "unbiased" machine learning system.
icon: FaPaw
---

import {ReactIcon} from  "components/Icons"

Whenever people ask me whether it is possible to make an unbiased machine learning system, I strain to explain that the answer depends on the intent of the question. We can make "good" machine learning systems, but the problem of priors means we will never make an "unbiased" machine learning system.

If you know the [sun is highly unlikely to stop generating light](https://xkcd.com/1132/), then you will have confidence the sun will rise in the morning. The expectation that the sun will rise is a "prior" belief in statistics and it is an immensely important statistical property in machine learning. It is also an extremely important one in the ethics and safety communities. To an ethicist, priors can perpetuate a past that is typically unrepresentative of the world we would like to build together. Many incidents in the [AI Incident Database](https://incidentdatabase.ai/) have this property, including those in [criminal justice](https://incidentdatabase.ai/cite/40/), [college admissions](https://incidentdatabase.ai/cite/135/), and [beauty pageants](https://incidentdatabase.ai/cite/49/). Similarly, when an intelligent system operates outside prior beliefs, it also produces incidents as in the case of an [algorithmic buyer of real estate nearly bankrupting itself](https://incidentdatabase.ai/cite/149/). All intelligent systems involve priors in one form or another -- they are inescapable.

Given how important priors are to system performance, ethics, and safety, arriving at an intuition for the priors embedded in intelligent systems is immensely important.

My goal in this blog post is not to sort through the ethics and safety of various priors. Instead, I am going to present a series of human-centered examples where prior beliefs greatly influence human perception and relate those observations to intelligent systems currently in the world. The examples provide intuition for why a machine learning system will sometimes perform surprisingly poorly in situations where the prior beliefs of the system are violated.

**Audio Priors**

Let's begin with speech recognition, which is the problem of identifying spoken words. When shipping products at [Syntiant](https://www.syntiant.com/), we would occasionally receive reports from the field where devices would activate under unexpected circumstances. To demonstrate the fine line and human biases, I liked to show this video:

*(Watch the video twice, but start reading the prompts in the video from different points)*

<iframe width="560" height="315" src="https://www.youtube.com/embed/JbueQpmh_8A" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

When I listen to the audio, I hear whatever words I am reading at that moment. This is because my human audio and visual systems are integrated to better understand the physical world. My audio perception of the world is thus colored by the history of my experiences leading up to the time I am listening to the audio. I am constantly updating my prior belief about what sounds I am likely hearing based on things that are not even producing the sound. The effect is so profound, that my mind doesn't just change my interpretation of what was heard -- the sounds themselves transform to conform to my brain's understanding of the sound. The sound is pysically consistent but my experience of it changes. Priors are important for sensing the world.

*(begin informed speculation)* Audio classification companies can get into trouble when they don't state the priors with which they are operating. A frequent [entrant](https://incidentdatabase.ai/entities/shotspotter/) into the AI Incident Database is the gunshot detection company [ShotSpotter](https://www.shotspotter.com/), which has gotten into trouble for allegedly false claims regarding gunshots in a variety of cities. I [do not believe](https://twitter.com/seanmcgregor/status/1575241420239425538?s=20) they have the capacity to reliably differentiate gunshots from many similar sounds. For a sufficiently loud high-impulse sound, their prior belief will indicate "gunshot" 364 days of the year in the United States; on Independence Day, they likely need to report vastly fewer gunshot candidates to police by updating their prior belief that a sounds is a gunshot. These priors would be explored in any court room where ShotSpotter's accuracy is examined, which is why they may have repeatedly avoided entering ShotSpotter detections into evidence. *(end informed speculation)*

To be clear, ShotSpotter has much to offer in community policing. Quickly dispatching police to the site of a mass shooting could save a great many lives. However, the disconnect between systems like ShotSpotter and how such systems may be viewed by law enforcement (i.e., an infallible ear in the sky) can make them dangerous. Although it would sell fewer systems, "ShotSpotter" may more appropriately be named "ImpulseSpotter." People would behave more appropriately in response to system detections: they would operate with an appropriate prior belief for what they will find at the alert site.

**Visual Priors**

The next human example is the [Stroop Effect](https://en.wikipedia.org/wiki/Stroop_effect), which is a [test](https://itservices.cas.unt.edu/~tam/SelfTests/StroopEffects.html) where people are asked to identify the color of written words spelling an unrelated color. It is quite difficult to quickly identify the color and people often say the wrong color. Here people can process the written word before identifying the color rendered with the word, which then changes the prior expectation for what word they need to say. This is a problem also exhibited in vision-based self-driving car systems such as Tesla cars. Without multiple modalities (e.g., vision, lidar, radar, audio, etc.), the prior belief cannot be checked against an independent sensory input. However, in safety critical systems like driving systems, you can't adopt a high prior belief that pedestrians will stay on sidewalks since it will result in deaths. Uber "solved" the problem by relying on human backup drivers and turned off the automatic safety systems, which produced a [death](https://incidentdatabase.ai/cite/4/). Tesla keeps the safety system active, but in so doing accepted a rate of [phantom breaking](https://incidentdatabase.ai/cite/208/), which they are unlikely to solve solely with their vision system.

<figure>
  <a target="_blank" href="/static/images/blog/content/Stroop_comparison.png">
    <img style={{ maxWidth: 500 }} width="100%" src="/static/images/blog/content/Stroop_comparison.png" />
  </a>
  <figcaption>Words for measuring the Stroop effect. The top row has words matching their colors while the bottom row has disagreeing words and their colors. People can identify the colors of the top row far faster than the colors in the bottom row.</figcaption>
</figure>

**Social Priors**

Society also develops collective priors as conventions. What is this symbol?

<ReactIcon i="FaPaw" size={100} />

Most people would say it is a dog footprint. However, such prints can also be made by many species that are not dogs. We know this is a dog footprint only because our prior expectation is that this is a dog footprint. Computer systems that are not explicitly trained with these social priors will have difficulty operating in human environments. Consequently, many academics have argued the necessity for "embodied" artificial intelligence, because that shared experience would allow for producing more human-like priors. Herein lies the problem for both the current systems like ChatGPT and any future embodied system: it will need to have a culture as a collection of prior beliefs. The dog print will need to be a dog print where it is understood as such, and it will need to be a wolf print where it carries that meaning. Our ability to understand the internal state of a system will then need to be informed by priors on the nature of the system's cultural targets. We need priors both in the construction of machine learning systems, and in our human understanding of a growing collection of AI systems.

**Wrap Up**

Many have called for embedding some notion of common sense in AI systems. They argue for a universal truth of knowledge and reason. However, high dimensional systems (e.g., systems that work with sound and images), do not work without some prior notions...

Priors are power. Priors are value. Priors are repression. and Priors are safety. Without them, these systems will not function. and with them, we are destined to forever fight over the configuration of these systems.