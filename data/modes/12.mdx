---
number: 12
dateAdded: '2025-04-01'
dateUpdated: '2025-04-01'
short: Prompt writers bias the sample to their own demographically-aligned word use,
  topics of interest, or other dimensions that tend to not explore the entirety of
  supported input space for the benchmark's supported use case
example: A benchmark for evaluating open-domain question answering is written by a
  small group of university-educated crowdworkers in the U.S. Their prompts disproportionately
  reflect Western pop culture, academic language, and topics of interest like sports,
  entertainment, and politics from a U.S. perspective. The SUT performs well on the
  benchmark, but when deployed globally, it struggles to handle queries about regional
  histories, idioms, or culturally specific contexts. A benchmark user in a multinational
  organization selects this SUT based on its top score, only to discover it performs
  poorly in non-Western markets and erodes user trust due to its perceived cultural
  bias.
severity: 0.6
stage: (2) Prompt Generation
dimension: Comprehensiveness
about: todo
---

todo