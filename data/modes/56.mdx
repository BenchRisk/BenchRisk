---
number: 56
dateAdded: '2025-04-01'
dateUpdated: '2025-04-01'
short: The benchmark authors do not know how to formulate the problem as prompts that
  are illustrative to the user relying on the benchmark
example: A benchmark aims to evaluate a language model's ability to assist with complex
  project planning. However, the benchmark authors, lacking deep expertise in project
  management (i.e., they are not domain experts), create prompts that are overly simplistic,
  focusing on isolated sub-tasks with clear, unambiguous instructions. A user looking
  to employ an LLM for real-world project planning faces messy, ill-defined problems
  with conflicting priorities and the need for nuanced decision-making. The top-performing
  model on the benchmark excels at the straightforward tasks presented but falters
  significantly when confronted with the ambiguity and complexity of real-world project
  scenarios. The user, misled by the benchmark's seemingly relevant task, selects
  a model that ultimately proves unhelpful for their actual needs because the benchmark
  prompts failed to capture the essential challenges of project planning as experienced
  in practice.
severity: 0.0
stage: (1) Task Definition
dimension: Correctness
about: todo
---

todo