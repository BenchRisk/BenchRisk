---
number: 27
dateAdded: '2025-04-01'
dateUpdated: '2025-04-01'
short: Low interrater reliability of ground truth data used to tune the evaluator
example: The benchmark uses human annotators to generate ground truth labels or scores
  that are later used to train an automated evaluator. However, the annotators frequently
  disagree on task success criteria—such as what constitutes a "correct," "helpful,"
  or "safe" response—due to vague instructions, subjective judgments, or cultural
  differences. This results in low interrater reliability, with inconsistent and noisy
  labels forming the basis of the evaluator’s training data. As a result, the evaluator
  itself becomes unreliable, often reflecting annotator bias or randomness rather
  than objective quality. A benchmark user, unaware of this underlying inconsistency,
  trusts the evaluator’s scores and selects a system that performs well on flawed
  metrics but poorly in actual deployment scenarios.
severity: 0.6
stage: (4) Output Evaluation
dimension: Consistency
about: todo
---

todo