---
number: 46
dateAdded: '2025-04-01'
dateUpdated: '2025-04-01'
short: SUT developers can run the benchmark an unlimited number of times
example: A benchmark allows SUT developers to submit their models for evaluation as
  many times as they wish. The developers of "NovaMind" repeatedly run the benchmark,
  meticulously analyzing the failure cases after each run. They then fine-tune their
  model specifically to improve its performance on the exact prompts and evaluation
  metrics of the benchmark, without necessarily improving its generalization capabilities
  on unseen data. A benchmark user, seeing the consistently high scores of "NovaMind,"
  selects it believing it to be a robust and generally capable model. However, in
  real-world applications with slightly different inputs or evaluation criteria, "NovaMind"
  underperforms significantly because its apparent success was largely due to overfitting
  to the specific nuances of the benchmark.
severity: 0.8
stage: (3) Prompt Inferencing
dimension: Longevity
about: todo
---

todo