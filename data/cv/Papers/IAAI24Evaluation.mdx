---
title: "AI Evaluation Authorities: A Case Study Mapping Model Audits to Persistent Standards"
venue: The Thirty-Sixth Annual Conference on Innovative Applications of Artificial Intelligence (IAAI-24)
authors: Chadda, A., McGregor, S., Hostetler, J.
citation: "Chadda, Arihant, et al. \"AI Evaluation Authorities: A Case Study Mapping Model Audits to Persistent Standards.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 38. No. 21. 2024."
bibtex: "@inproceedings{chadda2024ai, title={AI Evaluation Authorities: A Case Study Mapping Model Audits to Persistent Standards},author={Chadda, Arihant and McGregor, Sean and Hostetler, Jesse and Brennen, Andrea},booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},volume={38},number={21},pages={23035--23040},year={2024}}"
year: "2024"
demo: https://dyff.io/
paper: https://ojs.aaai.org/index.php/AAAI/article/view/30346
hide_tags: cv-tag-conference cv-tag-talk cv-tag-poster cv-tag-peer-reviewed cv-tag-ml cv-tag-application cv-tag-safety
highlight: true
---
This study was the start of an agenda looking at how we can produce standards-as-code (or, in the case of machine learning systems, data). It has developed into the
assessment platform Dyff.

**Abstract**

> Intelligent system audits are labor-intensive assurance activities that are typically performed once and discarded along with the opportunity to programmatically test all similar products for the market. This study illustrates how several incidents (i.e., harms) involving Named Entity Recognition (NER) can be prevented by scaling up a previously-performed audit of NER systems. The audit instrument's diagnostic capacity is maintained through a security model that protects the underlying data (i.e., addresses Goodhart's Law). An open-source evaluation infrastructure is released along with an example derived from a real-world audit that reports aggregated findings without exposing the underlying data.
