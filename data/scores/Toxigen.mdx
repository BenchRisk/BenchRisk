---
name: Toxigen
adoptedMitigations:
  - 1
  - 106
  - 3
  - 70
  - 140
  - 5
  - 101
  - 141
  - 11
  - 142
  - 12
  - 84
  - 85
  - 13
  - 14
  - 79
  - 19
  - 22
  - 92
  - 94
  - 137
  - 135
  - 55
  - 138
  - 139
  - 33
  - 34
  - 35
  - 122
  - 123
  - 38
  - 88
  - 82
  - 127
  - 104
  - 105
  - 103
  - 107
  - 128
  - 129
  - 130
  - 131
  - 132
  - 124
  - 125
  - 126
  - 133
  - 136
absentMitigations:
  - 186
  - 198
  - 64
  - 162
  - 68
  - 187
  - 188
  - 189
  - 190
  - 191
  - 192
  - 2
  - 163
  - 4
  - 199
  - 89
  - 6
  - 8
  - 7
  - 9
  - 102
  - 10
  - 62
  - 164
  - 69
  - 178
  - 165
  - 86
  - 183
  - 184
  - 87
  - 90
  - 65
  - 71
  - 144
  - 17
  - 193
  - 143
  - 80
  - 201
  - 145
  - 18
  - 146
  - 179
  - 147
  - 180
  - 166
  - 21
  - 194
  - 134
  - 195
  - 148
  - 181
  - 23
  - 185
  - 63
  - 24
  - 25
  - 72
  - 26
  - 27
  - 91
  - 149
  - 53
  - 150
  - 151
  - 73
  - 28
  - 93
  - 46
  - 196
  - 47
  - 81
  - 152
  - 29
  - 48
  - 153
  - 182
  - 30
  - 49
  - 154
  - 95
  - 31
  - 32
  - 50
  - 96
  - 97
  - 155
  - 54
  - 202
  - 156
  - 98
  - 58
  - 99
  - 60
  - 157
  - 59
  - 100
  - 61
  - 158
  - 51
  - 159
  - 200
  - 56
  - 36
  - 160
  - 197
  - 75
  - 37
  - 66
  - 83
  - 57
  - 76
  - 39
  - 42
  - 77
  - 78
  - 44
  - 74
  - 43
  - 45
  - 161
  - 52
  - 67
  - 108
  - 109
  - 112
  - 113
  - 114
  - 118
  - 110
  - 115
  - 116
  - 117
  - 119
  - 111
  - 120
  - 121
benchmarkDescription: Benchmark indicates whether a system detects implicit hate speech targeting minority groups while avoiding false alarms
dateScored: '2025-04-01'
reference: https://arxiv.org/abs/2203.09509
---

ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection

> Toxic language detection systems often falsely flag text that contains minority group mentions as toxic, as those groups are often the targets of online hate. Such over-reliance on spurious correlations also causes systems to struggle with detecting implicitly toxic language. To help mitigate these issues, we create ToxiGen, a new large-scale and machine-generated dataset of 274k toxic and benign statements about 13 minority groups. We develop a demonstration-based prompting framework and an adversarial classifier-in-the-loop decoding method to generate subtly toxic and benign text with a massive pretrained language model. Controlling machine generation in this way allows ToxiGen to cover implicitly toxic text at a larger scale, and about more demographic groups, than previous resources of human-written text. We conduct a human evaluation on a challenging subset of ToxiGen and find that annotators struggle to distinguish machine-generated text from human-written language. We also find that 94.5% of toxic examples are labeled as hate speech by human annotators. Using three publicly-available datasets, we show that finetuning a toxicity classifier on our data improves its performance on human-written data substantially. We also demonstrate that ToxiGen can be used to fight machine-generated toxicity as finetuning improves the classifier significantly on our evaluation subset. Our code and data can be found at [this https URL.](https://github.com/microsoft/ToxiGen)
