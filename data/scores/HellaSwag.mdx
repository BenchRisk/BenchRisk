---
name: HellaSwag
adoptedMitigations:
  - 1
  - 64
  - 68
  - 89
  - 70
  - 6
  - 62
  - 69
  - 142
  - 84
  - 85
  - 87
  - 14
  - 71
  - 80
  - 48
  - 153
  - 30
  - 135
  - 154
  - 96
  - 97
  - 98
  - 99
  - 100
  - 33
  - 122
  - 123
  - 82
  - 108
  - 109
  - 128
  - 129
  - 130
  - 132
  - 124
  - 125
  - 126
absentMitigations:
  - 186
  - 198
  - 106
  - 162
  - 187
  - 188
  - 189
  - 190
  - 191
  - 192
  - 2
  - 3
  - 4
  - 163
  - 199
  - 140
  - 5
  - 8
  - 101
  - 7
  - 9
  - 141
  - 102
  - 10
  - 11
  - 164
  - 165
  - 12
  - 178
  - 86
  - 183
  - 184
  - 13
  - 90
  - 65
  - 144
  - 17
  - 193
  - 143
  - 201
  - 145
  - 18
  - 79
  - 146
  - 19
  - 179
  - 147
  - 180
  - 166
  - 21
  - 194
  - 134
  - 22
  - 195
  - 148
  - 181
  - 23
  - 185
  - 92
  - 63
  - 24
  - 25
  - 72
  - 26
  - 27
  - 91
  - 149
  - 53
  - 150
  - 151
  - 73
  - 28
  - 93
  - 46
  - 81
  - 196
  - 152
  - 29
  - 94
  - 47
  - 137
  - 182
  - 49
  - 31
  - 95
  - 32
  - 50
  - 155
  - 54
  - 202
  - 156
  - 55
  - 138
  - 58
  - 157
  - 60
  - 59
  - 139
  - 61
  - 158
  - 34
  - 51
  - 159
  - 200
  - 35
  - 56
  - 36
  - 160
  - 197
  - 75
  - 37
  - 38
  - 88
  - 66
  - 83
  - 127
  - 57
  - 76
  - 39
  - 42
  - 77
  - 78
  - 44
  - 74
  - 43
  - 45
  - 161
  - 52
  - 67
  - 104
  - 105
  - 103
  - 107
  - 112
  - 113
  - 114
  - 118
  - 110
  - 115
  - 116
  - 117
  - 119
  - 111
  - 120
  - 121
  - 131
  - 133
  - 136
benchmarkDescription: Benchmark capacity to reason about physical and social concepts by prompting for sentence endings from challenging, adversarially filtered alternatives, providing users with a measure of the scope of model capacity.
dateScored: '2025-04-01'
reference: https://arxiv.org/abs/1905.07830
---

HellaSwag: Can a Machine Really Finish Your Sentence?

> Recent work by Zellers et al. (2018) introduced a new task of commonsense natural language inference: given an event description such as "A woman sits at a piano," a machine must select the most likely followup: "She sets her fingers on the keys." With the introduction of BERT, near human-level performance was reached. Does this mean that machines can perform human level commonsense inference?
> In this paper, we show that commonsense inference still proves difficult for even state-of-the-art models, by presenting HellaSwag, a new challenge dataset. Though its questions are trivial for humans (>95% accuracy), state-of-the-art models struggle (<48%). We achieve this via Adversarial Filtering (AF), a data collection paradigm wherein a series of discriminators iteratively select an adversarial set of machine-generated wrong answers. AF proves to be surprisingly robust. The key insight is to scale up the length and complexity of the dataset examples towards a critical 'Goldilocks' zone wherein generated text is ridiculous to humans, yet often misclassified by state-of-the-art models.
> Our construction of HellaSwag, and its resulting difficulty, sheds light on the inner workings of deep pretrained models. More broadly, it suggests a new path forward for NLP research, in which benchmarks co-evolve with the evolving state-of-the-art in an adversarial way, so as to present ever-harder challenges.
