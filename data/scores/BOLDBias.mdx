---
name: BOLDBias
adoptedMitigations:
  - 1
  - 106
  - 64
  - 89
  - 70
  - 6
  - 10
  - 62
  - 69
  - 84
  - 86
  - 85
  - 87
  - 145
  - 18
  - 22
  - 48
  - 137
  - 30
  - 135
  - 51
  - 159
  - 122
  - 123
  - 82
  - 128
  - 129
  - 130
  - 131
  - 132
  - 124
  - 125
  - 126
absentMitigations:
  - 186
  - 198
  - 162
  - 68
  - 187
  - 188
  - 189
  - 190
  - 191
  - 192
  - 2
  - 3
  - 4
  - 163
  - 199
  - 140
  - 5
  - 8
  - 101
  - 7
  - 9
  - 141
  - 102
  - 11
  - 164
  - 142
  - 178
  - 12
  - 165
  - 183
  - 184
  - 13
  - 90
  - 14
  - 65
  - 71
  - 144
  - 17
  - 193
  - 143
  - 80
  - 201
  - 79
  - 146
  - 19
  - 179
  - 147
  - 180
  - 166
  - 21
  - 194
  - 134
  - 195
  - 148
  - 181
  - 23
  - 185
  - 92
  - 63
  - 24
  - 25
  - 72
  - 26
  - 27
  - 91
  - 149
  - 53
  - 150
  - 151
  - 73
  - 28
  - 93
  - 46
  - 81
  - 196
  - 152
  - 29
  - 94
  - 47
  - 153
  - 182
  - 49
  - 154
  - 95
  - 31
  - 32
  - 50
  - 96
  - 97
  - 155
  - 54
  - 202
  - 98
  - 156
  - 55
  - 99
  - 58
  - 138
  - 60
  - 157
  - 100
  - 59
  - 139
  - 61
  - 158
  - 33
  - 34
  - 200
  - 35
  - 56
  - 36
  - 160
  - 197
  - 75
  - 37
  - 38
  - 88
  - 66
  - 83
  - 127
  - 57
  - 76
  - 39
  - 42
  - 77
  - 78
  - 44
  - 74
  - 43
  - 45
  - 161
  - 52
  - 67
  - 104
  - 105
  - 103
  - 107
  - 108
  - 109
  - 112
  - 113
  - 114
  - 118
  - 110
  - 115
  - 116
  - 117
  - 119
  - 111
  - 120
  - 121
  - 133
  - 136
benchmarkDescription: Benchmark indicates whether an open-ended text generation model maintains fairness and avoids social biases across domains such as profession, gender, race, religion, and ideology when prompted with real-world English contexts, helping users evaluate the modelâ€™s equity and ethical behavior.
dateScored: '2025-04-01'
reference: https://arxiv.org/abs/2101.11718
---

BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation

> Recent advances in deep learning techniques have enabled machines to generate cohesive open-ended text when prompted with a sequence of words as context. While these models now empower many downstream applications from conversation bots to automatic storytelling, they have been shown to generate texts that exhibit social biases. To systematically study and benchmark social biases in open-ended language generation, we introduce the Bias in Open-Ended Language Generation Dataset (BOLD), a large-scale dataset that consists of 23,679 English text generation prompts for bias benchmarking across five domains: profession, gender, race, religion, and political ideology. We also propose new automated metrics for toxicity, psycholinguistic norms, and text gender polarity to measure social biases in open-ended text generation from multiple angles. An examination of text generated from three popular language models reveals that the majority of these models exhibit a larger social bias than human-written Wikipedia text across all domains. With these results we highlight the need to benchmark biases in open-ended language generation and caution users of language generation models on downstream tasks to be cognizant of these embedded prejudices.
