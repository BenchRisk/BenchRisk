---
name: AIRBench
adoptedMitigations:
  - 1
  - 68
  - 3
  - 5
  - 101
  - 102
  - 10
  - 12
  - 13
  - 14
  - 92
  - 94
  - 135
  - 55
  - 51
  - 38
  - 88
  - 104
  - 105
  - 103
  - 128
  - 129
  - 130
  - 131
  - 132
  - 124
  - 125
  - 126
  - 133
  - 136
absentMitigations:
  - 186
  - 198
  - 106
  - 64
  - 162
  - 187
  - 188
  - 189
  - 190
  - 191
  - 192
  - 2
  - 4
  - 163
  - 70
  - 199
  - 89
  - 140
  - 6
  - 8
  - 7
  - 9
  - 141
  - 62
  - 11
  - 164
  - 69
  - 142
  - 165
  - 178
  - 84
  - 86
  - 183
  - 184
  - 85
  - 87
  - 90
  - 65
  - 71
  - 144
  - 17
  - 193
  - 143
  - 80
  - 145
  - 18
  - 201
  - 79
  - 146
  - 19
  - 179
  - 147
  - 180
  - 166
  - 21
  - 194
  - 134
  - 22
  - 195
  - 148
  - 181
  - 23
  - 185
  - 63
  - 24
  - 25
  - 72
  - 26
  - 27
  - 91
  - 149
  - 53
  - 150
  - 151
  - 73
  - 28
  - 93
  - 46
  - 196
  - 47
  - 81
  - 152
  - 29
  - 137
  - 48
  - 182
  - 153
  - 30
  - 49
  - 154
  - 95
  - 31
  - 32
  - 50
  - 96
  - 97
  - 155
  - 54
  - 202
  - 156
  - 98
  - 99
  - 58
  - 138
  - 60
  - 157
  - 100
  - 59
  - 139
  - 61
  - 158
  - 33
  - 34
  - 159
  - 200
  - 35
  - 56
  - 36
  - 160
  - 197
  - 75
  - 37
  - 122
  - 123
  - 66
  - 83
  - 82
  - 127
  - 57
  - 76
  - 39
  - 42
  - 77
  - 78
  - 44
  - 74
  - 43
  - 45
  - 161
  - 52
  - 67
  - 107
  - 108
  - 109
  - 112
  - 113
  - 114
  - 118
  - 110
  - 115
  - 116
  - 117
  - 119
  - 111
  - 120
  - 121
benchmarkDescription: Benchmark evaluates LLM safety by measuring responses to prompts that are mapped to granular risk categories derived from government regulations and AI policies.
dateScored: '2025-04-01'
reference: https://arxiv.org/abs/2407.17436v2
---

AIR-Bench 2024: A Safety Benchmark Based on Risk Categories from Regulations and Policies

> Foundation models (FMs) provide societal benefits but also amplify risks. Governments, companies, and researchers have proposed regulatory frameworks, acceptable use policies, and safety benchmarks in response. However, existing public benchmarks often define safety categories based on previous literature, intuitions, or common sense, leading to disjointed sets of categories for risks specified in recent regulations and policies, which makes it challenging to evaluate and compare FMs across these benchmarks. To bridge this gap, we introduce AIR-Bench 2024, the first AI safety benchmark aligned with emerging government regulations and company policies, following the regulation-based safety categories grounded in our AI risks study, AIR 2024. AIR 2024 decomposes 8 government regulations and 16 company policies into a four-tiered safety taxonomy with 314 granular risk categories in the lowest tier. AIR-Bench 2024 contains 5,694 diverse prompts spanning these categories, with manual curation and human auditing to ensure quality. We evaluate leading language models on AIR-Bench 2024, uncovering insights into their alignment with specified safety concerns. By bridging the gap between public benchmarks and practical AI risks, AIR-Bench 2024 provides a foundation for assessing model safety across jurisdictions, fostering the development of safer and more responsible AI systems.
