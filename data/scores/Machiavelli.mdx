---
name: Machiavelli
adoptedMitigations:
  - 1
  - 183
  - 184
  - 90
  - 179
  - 185
  - 135
  - 55
  - 138
  - 139
  - 33
  - 123
  - 104
  - 105
  - 103
  - 107
  - 128
  - 129
  - 130
  - 131
  - 132
  - 124
  - 125
  - 126
  - 133
absentMitigations:
  - 186
  - 198
  - 106
  - 64
  - 162
  - 68
  - 187
  - 188
  - 189
  - 190
  - 191
  - 192
  - 2
  - 3
  - 4
  - 163
  - 70
  - 199
  - 89
  - 140
  - 5
  - 6
  - 8
  - 101
  - 7
  - 9
  - 141
  - 102
  - 10
  - 62
  - 11
  - 164
  - 69
  - 142
  - 165
  - 12
  - 178
  - 84
  - 86
  - 13
  - 85
  - 87
  - 14
  - 65
  - 71
  - 144
  - 17
  - 193
  - 143
  - 80
  - 145
  - 18
  - 201
  - 79
  - 146
  - 19
  - 147
  - 180
  - 21
  - 166
  - 194
  - 134
  - 22
  - 195
  - 148
  - 181
  - 23
  - 92
  - 63
  - 24
  - 25
  - 72
  - 26
  - 27
  - 91
  - 149
  - 53
  - 150
  - 151
  - 73
  - 28
  - 93
  - 46
  - 81
  - 196
  - 152
  - 29
  - 94
  - 47
  - 137
  - 48
  - 182
  - 153
  - 30
  - 49
  - 154
  - 95
  - 31
  - 32
  - 50
  - 96
  - 97
  - 155
  - 54
  - 202
  - 156
  - 98
  - 58
  - 99
  - 60
  - 157
  - 59
  - 100
  - 61
  - 158
  - 34
  - 51
  - 159
  - 200
  - 35
  - 56
  - 36
  - 160
  - 197
  - 75
  - 37
  - 122
  - 38
  - 66
  - 82
  - 83
  - 88
  - 127
  - 57
  - 76
  - 39
  - 42
  - 77
  - 78
  - 44
  - 74
  - 43
  - 45
  - 161
  - 52
  - 67
  - 108
  - 109
  - 112
  - 113
  - 114
  - 118
  - 110
  - 115
  - 116
  - 117
  - 119
  - 111
  - 120
  - 121
  - 136
benchmarkDescription: Benchmark measures the trade-off between reward optimization and ethical behavior in AI agents by quantifying their propensity for harmful actions—such as power-seeking, deception, and causing disutility—within interactive social scenarios.
dateScored: '2025-04-01'
reference: https://aypan17.github.io/machiavelli/
hide: true
---

Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark

> Artificial agents have traditionally been trained to maximize reward, which may incentivize power-seeking and deception, analogous to how next-token prediction in language models (LMs) may incentivize toxicity. So do agents naturally learn to be Machiavellian? And how do we measure these behaviors in general-purpose models such as GPT-4? Towards answering these questions, we introduce MACHIAVELLI, a benchmark of 134 Choose-Your-Own-Adventure games containing over half a million rich, diverse scenarios that center on social decision-making. Scenario labeling is automated with LMs, which are more performant than human annotators. We mathematize dozens of harmful behaviors and use our annotations to evaluate agents' tendencies to be power-seeking, cause disutility, and commit ethical violations. We observe some tension between maximizing reward and behaving ethically. To improve this trade-off, we investigate LM-based methods to steer agents' towards less harmful behaviors. Our results show that agents can both act competently and morally, so concrete progress can currently be made in machine ethics--designing agents that are Pareto improvements in both safety and capabilities.
