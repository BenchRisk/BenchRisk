---
name: DecodingTrustPrivacy
adoptedMitigations:
  - 1
  - 64
  - 89
  - 10
  - 69
  - 12
  - 13
  - 14
  - 92
  - 53
  - 94
  - 153
  - 154
  - 157
  - 158
  - 123
  - 104
  - 103
  - 128
  - 129
  - 130
  - 132
  - 124
  - 125
  - 126
  - 133
absentMitigations:
  - 186
  - 198
  - 106
  - 162
  - 68
  - 187
  - 188
  - 189
  - 190
  - 191
  - 192
  - 2
  - 3
  - 4
  - 163
  - 70
  - 199
  - 140
  - 5
  - 6
  - 8
  - 101
  - 7
  - 9
  - 141
  - 102
  - 62
  - 11
  - 164
  - 142
  - 165
  - 178
  - 84
  - 86
  - 183
  - 184
  - 85
  - 87
  - 90
  - 65
  - 71
  - 144
  - 17
  - 193
  - 143
  - 80
  - 145
  - 18
  - 201
  - 79
  - 146
  - 19
  - 179
  - 147
  - 180
  - 166
  - 21
  - 194
  - 134
  - 22
  - 195
  - 148
  - 181
  - 23
  - 185
  - 63
  - 24
  - 25
  - 72
  - 26
  - 27
  - 91
  - 149
  - 150
  - 151
  - 73
  - 28
  - 93
  - 46
  - 196
  - 47
  - 81
  - 152
  - 29
  - 135
  - 137
  - 48
  - 182
  - 30
  - 49
  - 31
  - 95
  - 32
  - 50
  - 96
  - 97
  - 155
  - 54
  - 202
  - 98
  - 156
  - 55
  - 138
  - 99
  - 58
  - 60
  - 59
  - 139
  - 100
  - 61
  - 33
  - 34
  - 51
  - 159
  - 200
  - 35
  - 56
  - 36
  - 160
  - 197
  - 75
  - 37
  - 122
  - 38
  - 66
  - 82
  - 83
  - 88
  - 127
  - 57
  - 76
  - 39
  - 42
  - 77
  - 78
  - 44
  - 74
  - 43
  - 45
  - 161
  - 52
  - 67
  - 105
  - 107
  - 108
  - 109
  - 112
  - 113
  - 114
  - 118
  - 110
  - 115
  - 116
  - 117
  - 119
  - 111
  - 120
  - 121
  - 131
  - 136
benchmarkDescription: Benchmark evaluates language models (specifically GPT models) privacy by looking into if they leas sesitive training data, reveal private user inputs and understand non pii related private information in conversations.
dateScored: '2025-04-01'
reference: https://decodingtrust.github.io
hide: true
---

> DecodingTrust aims at providing a thorough assessment of trustworthiness in GPT models.

> This research endeavor is designed to help researchers and practitioners better understand the capabilities, limitations, and potential risks involved in deploying these state-of-the-art Large Language Models (LLMs).

> This project is organized around the following eight primary perspectives of trustworthiness, including:

> * Toxicity
> * Stereotype and bias
> * Adversarial robustness
> * Out-of-Distribution Robustness
> * Privacy
> * Robustness to Adversarial Demonstrations
> * Machine Ethics
> * Fairness