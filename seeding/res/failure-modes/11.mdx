---
number: 11
dateAdded: '2025-04-01'
dateUpdated: '2025-04-01'
short: Prompts focus on adversarial users (e.g., users are attempting to circumvent
  a guard model)
example: A benchmark evaluating the safety of a language model focuses exclusively
  on adversarial jailbreak attemptsâ€”e.g., users trying to trick the model into giving
  instructions for dangerous activities. It ignores benign but naive users who unintentionally
  elicit harmful responses due to ambiguous phrasing or lack of domain knowledge.
  A model is trained and evaluated solely on its ability to block adversarial attacks
  and scores highly. However, when deployed in a public helpdesk context, it frequently
  outputs unsafe or misleading content to sincere users with poorly worded or misunderstood
  queries. The benchmark user believes the model is safe and deploys it broadly, exposing
  end users to unanticipated risks.
severity: 0.6
stage: (2) Prompt Generation
dimension: Comprehensiveness
about: todo
---

todo