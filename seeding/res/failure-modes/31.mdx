---
number: 31
dateAdded: '2025-04-01'
dateUpdated: '2025-04-01'
short: Evaluator(s) perform poorly across all SUTs
example: A benchmark employs undergraduate psychology students with no technical background
  to evaluate advanced AI reasoning on complex mathematical proofs. These evaluators
  cannot distinguish between correct proofs and those with subtle errors, giving high
  scores to any response that looks sophisticated regardless of accuracy. As a result,
  all tested systems receive similarly high scores despite vast differences in actual
  mathematical reasoning capabilities. The benchmark user selects the cheapest SUT
  thinking it performs equivalently to premium options, then deploys it to verify
  critical engineering calculations for a bridge design, resulting in structural flaws
  being missed and necessitating costly redesigns when discovered by other verification
  methods.
severity: 0.8
stage: (4) Output Evaluation
dimension: Consistency
about: todo
---

todo