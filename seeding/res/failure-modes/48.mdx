---
number: 48
dateAdded: '2025-04-01'
dateUpdated: '2025-04-01'
short: Users cannot map the scores to a mental model of likely SUT behavior in the
  real world
example: A benchmark provides a highly abstract "coherence score" for a language model's
  long-form generation, calculated using a complex combination of statistical metrics
  like perplexity and cosine similarity of embeddings. While Model A achieves a score
  of 0.92 and Model B scores 0.88, a user struggles to understand what these numbers
  practically mean for how the models will perform when generating a business report
  or a creative short story. They have no intuitive sense of the difference in quality
  or the types of coherence failures they might encounter with either model in real-world
  use. Consequently, their decision between Model A and Model B feels arbitrary, lacking
  a grounded understanding of how the benchmark scores translate to tangible differences
  in the models' output for their specific needs.
severity: 0.5
stage: (6) Grade Presentation
dimension: Intelligibility
about: todo
---

todo