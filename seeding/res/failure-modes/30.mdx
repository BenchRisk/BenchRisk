---
number: 30
dateAdded: '2025-04-01'
dateUpdated: '2025-04-01'
short: Certain SUTs produce outputs with higher evaluator errors than other SUTs
example: A benchmark evaluates AI coding assistants using human evaluators to judge
  solution correctness. One SUT generates visually clean, well-commented code with
  excellent formatting that appears professional but contains subtle logical errors.
  Another SUT produces messier but functionally correct code. Evaluators consistently
  rate the first SUT's visually appealing but buggy code higher, leading to inflated
  scores. The benchmark user selects this SUT for a critical software development
  project, resulting in deployed code with hidden bugs that only manifest in production
  environments, causing system failures and data corruption that take months to diagnose
  and fix.
severity: 0.6
stage: (4) Output Evaluation
dimension: Correctness
about: todo
---

todo