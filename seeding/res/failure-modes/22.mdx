---
number: 22
dateAdded: '2025-04-01'
dateUpdated: '2025-04-01'
short: Distribution of SUT inputs within the real world are substantially different
  in distribution from those within the benchmark (e.g., SUT users ask different questions
  from those posed by the benchmark authors)
example: A benchmark is designed by academic researchers to test a model’s ability
  to answer philosophical, scientific, and analytical questions with precise factual
  grounding. However, in a commercial deployment—such as a virtual assistant—users
  primarily ask casual, personal, or goal-directed questions (e.g., “What should I
  wear today?” or “Can you draft a message to my boss?”) that differ drastically in
  tone, content, and structure from the benchmark prompts. As a result, the model
  excels in benchmark evaluations but performs poorly in production. The benchmark
  user misinterprets the high benchmark scores as a signal of general utility and
  deploys the system in an environment where it routinely fails to meet user expectations.
severity: 0.6
stage: (3) Prompt Inferencing
dimension: Correctness
about: todo
---

todo