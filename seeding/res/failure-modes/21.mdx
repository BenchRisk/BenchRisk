---
number: 21
dateAdded: '2025-04-01'
dateUpdated: '2025-04-01'
short: Prompts are sent to model vendors when inferencing or all prompts are publicly
  available
example: The benchmark uses an API-based evaluation pipeline where prompts are sent
  directly to model vendors (e.g., OpenAI, Anthropic) for inference, or all prompts
  are published openly online. Model vendors are thus able to log, analyze, and optimize
  performance specifically on these benchmark prompts, either intentionally or as
  part of routine monitoring. This leads to inflated scores that do not reflect the
  modelsâ€™ generalization to unseen tasks. A benchmark user, unaware of this dynamic,
  interprets the scores as indicative of broader capability and deploys a system that
  underperforms on genuinely novel or proprietary tasks.
severity: 0.9
stage: (3) Prompt Inferencing
dimension: Longevity
about: todo
---

todo